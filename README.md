---
title: AI-Bookshelf RAG
emoji: ğŸ“š
colorFrom: blue
colorTo: indigo
sdk: docker
pinned: false
---

# AI-Bookshelf RAG

A Retrieval-Augmented Generation (RAG) system for managing and searching your personal book collection using vector embeddings and Milvus vector database.

## Features

- ğŸ“š **Browse Books** â€” View all embedded documents in your knowledge base
- ğŸ” **Semantic Search** â€” Ask questions and get AI-powered answers with source attribution
- â¬†ï¸ **Upload PDFs** â€” Add new books to the knowledge base (automatically chunked and embedded)
- ğŸ¤– **AI-Powered Responses** â€” Uses OpenAI-compatible LLM with retrieval-augmented generation
- ğŸ” **Vector Storage** â€” Milvus serverless vector database for fast similarity search

## Tech Stack

- **Frontend**: Streamlit
- **Backend**: Flask API
- **Vector DB**: Milvus (serverless)
- **Embeddings**: Sentence Transformers (all-mpnet-base-v2)
- **LLM**: OpenAI-compatible API (LiteLLM Proxy)
- **Container**: Docker

## Quick Start

### Local Development

1. Clone and setup:
```bash
cd AI-Bookshelf-RAG
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your credentials
```

2. Run both services locally:

**Option A: Start script (Windows)**
```powershell
start.bat
```

**Option B: Manual (all platforms)**
```bash
# Terminal 1 - Flask backend
python main.py

# Terminal 2 - Streamlit frontend
streamlit run ui.py  # http://localhost:8501
```

### Docker

```bash
docker build -t ai-bookshelf-rag .
docker run -p 8501:8501 -p 5000:5000 \
  -e API_KEY="your_key" \
  -e MILVUS_USERNAME="user" \
  -e MILVUS_PASSWORD="pass" \
  -e MILVUS_ENDPOINT="endpoint" \
  ai-bookshelf-rag
```

## Configuration

Set these environment variables in `.env`:

- `API_KEY` â€” OpenAI/LiteLLM API key
- `API_BASE_URL` â€” API endpoint
- `API_MODEL` â€” Model name
- `MILVUS_USERNAME`, `MILVUS_PASSWORD`, `MILVUS_ENDPOINT` â€” Database credentials
- `EMBEDDING_MODEL` â€” Sentence transformer (default: all-mpnet-base-v2)

See `.env.example` for all options.

## API Endpoints

**POST** `/query` â€” Search knowledge base
```json
{"query": "What are the main topics?"}
```

**GET** `/status` â€” Check system status

**POST** `/add-pdf` â€” Upload new PDF
```json
{"pdf_path": "pdf_references/book.pdf"}
```

## Deployment

### Hugging Face Spaces

1. Create a new Docker Space: https://huggingface.co/spaces
2. Add secrets in Space Settings (Repository secrets):
   - `API_KEY`
   - `MILVUS_USERNAME`
   - `MILVUS_PASSWORD`
   - `MILVUS_ENDPOINT`
3. Push code:
```bash
git remote add hf https://huggingface.co/spaces/YOUR_USERNAME/YOUR_SPACE_NAME
git push hf main
```

## Project Structure

```
â”œâ”€â”€ ui.py                      # Streamlit frontend
â”œâ”€â”€ main.py                    # Flask API backend
â”œâ”€â”€ config.py                  # Configuration
â”œâ”€â”€ embedding_utils.py         # Text embeddings
â”œâ”€â”€ milvus_manager.py          # Vector DB
â”œâ”€â”€ pdf_manager.py             # PDF processing
â”œâ”€â”€ ethical_layer.py           # LLM + safety
â”œâ”€â”€ Dockerfile                 # Container config
â”œâ”€â”€ start.sh / start.bat       # Launch scripts
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ .env.example              # Environment template
```

## License

MIT License

---

**Built with â¤ï¸ | RAG + AI + Vector Search**

### 3. Add Your PDFs
```bash
cp your_documents/*.pdf pdf_references/
```

### 4. Run Application
```bash
python main.py
```

The app will:
- Load embedding model
- Connect to Milvus
- Check for new PDFs
- Skip already-processed PDFs
- Start Flask API on http://localhost:5000

### 5. Query the API
```bash
curl -X POST http://localhost:5000/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Your question here"}'
```

## ğŸ“ Project Structure-AI-Part-
AI-Bookshelf(AI Part)
